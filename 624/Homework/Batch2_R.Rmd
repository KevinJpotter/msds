---
title: "Homework Set 2 in R"
author: "Zachary Palmore"
date: "7/8/2021"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Homework Set 2 in R 
DATA 624-01 Group 3 
Z. Palmore, K. Popkin, K. Potter, C. Nan, J. Ramalingam 

KJ 6.3
A chemical manufacturing process for a pharmaceutical product was discussed in Sect. 1.4. In this problem, the objective is to understand the relationship between biological measurements of the raw materials (predictors),measurements of the manufacturing process (predictors), and the response of
product yield. Biological predictors cannot be changed but can be used to assess the quality of the raw material before processing. On the other hand, manufacturing process predictors can be changed in the manufacturing process. Improving product yield by 1% will boost revenue by approximately
one hundred thousand dollars per batch:

Part A
Question
Start R and use these commands to load the data:
> library(AppliedPredictiveModeling)
> data(chemicalManufacturingProcess)
Code
```{r}
#Load the data
data(ChemicalManufacturingProcess)
df = data.frame(ChemicalManufacturingProcess)
head(df)
```
Response
The matrix processPredictors contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. yield contains the percent yield for each run.

Part B
Question
A small percentage of cells in the predictor set contain missing values. Use an imputation function to fill in these missing values (e.g., see Sect. 3.8).

Code
```{r}
#Fill in missing values with the median of each feature
for(i in 1:ncol(df)){
  df[is.na(df[,i]), i] <- median(df[,i], na.rm = TRUE)}
```

Response

Part C 
Question
Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter.
Code
```{r correlations}
#Filter to only Yield and sort in descending order
yield_corr = filter(melted_cormat_df, Var2 == "Yield")
yield_corr$absvalue = abs(yield_corr$value)
yield_corr2 = yield_corr[order(-yield_corr[,4]),]
yield_corr2
```

```{r modeling}
#Create the regression model
lmyield = lm(Yield~ManufacturingProcess32 + ManufacturingProcess36 + ManufacturingProcess09 + ManufacturingProcess13 + BiologicalMaterial02 + BiologicalMaterial06 + BiologicalMaterial03, data=train)
summary(lmyield)
```
Response
After splitting the data into a 75% training, 25% test datasets, I chose to build a linear regression model using the top 7 features based on the correlations.

Here are the correlations…

Here is the model.  The R2 of 61% isn’t very good and the only good predictor based on p-values and 0.05 cutoff is ManufacturingProcess32. 

Part D
Question
Predict the response for the test set.What is the value of the performance metric and how does this compare with the resampled performance metric on the training set?
Code
```{r}
#predict with the test data
predict(lmyield, newdata = test, interval = ‘prediction’)
```

```{r}
#Predict with the test data
lmyield_test = predict(lmyield, newdata = test, interval = 'prediction')
lmyield_test_df = data.frame(lmyield_test)
lmyield_test_df$actual = test$Yield
lmyield_test_df

lmyield_test_rmse = rmse(lmyield_test_df$actual, lmyield_test_df$fit)
cat('RMSE of the test data for this model is', lmyield_test_rmse)
```
Response
Using RMSE as the performance metric, the value if 1.25 as shown below…
The optimal value of the performance metric for the Test data is 1.25 as shown below…
Part E
Question
Which predictors are most important in the model you have trained? Do either the biological or process predictors dominate the list? 
Response
As shown in the above correlations list the first 7 most correlated features are 4 process features, followed by 3 biological features.  

In reviewing the regression model’s summary, the Process32 and Process36 are the most relevant p-values and the BiologicalMaterial02 feature is the third most relevant.  Note that only the Process32 feature has a p-value < 0.05. 
Part F
Question
Explore the relationships between each of the top predictors and the response. How could this information be helpful in improving yield in future runs of the manufacturing process?
Response
Per the previous exploration via correlations and p-values, the process features are topping the list of most relevant features.  This bodes well as these features can be changed (biological ones cannot change), so simulations could be done using the regression model and modified values.

Note that prior to experimenting with process feature value changes, more experimentation is needed on what features to use.  Having an Adjusted R2 on the training data of only 60% and having only one feature in the model summary with a p-value of 0.05 indicates there is room for improvement.  A good next step might be to build the model via forward stepwise or backward stepwise regression.  Or, training on some other model such as Random Forest, could also be used to identify which features are most relevant.

KJ 7.2
Friedman (1991) introduced several benchmark data sets created by simulation. One of these simulations used the following nonlinear equation to  create data:  y = 10 sin(πx1x2) + 20(x3 − 0.5)2 + 10x4 + 5x5 + N(0, σ2)  where the x values are random variables uniformly distributed between [0, 1]  (there are also 5 other non-informative variables also created in the simulation).The package mlbench contains a function called mlbench.friedman1 that simulates these data: 
> library(mlbench)
> set.seed(200)
> trainingData <- mlbench.friedman1(200, sd = 1)
> ## We convert the 'x' data from a matrix to a data frame
> ## One reason is that this will give the columns names.
> trainingData$x <- data.frame(trainingData$x)
> ## Look at the data using
> featurePlot(trainingData$x, trainingData$y)
> ## or other methods.
>
> ## This creates a list with a vector 'y' and a matrix
> ## of predictors 'x'. Also simulate a large test set to
> ## estimate the true error rate with good precision:
> testData <- mlbench.friedman1(5000, sd = 1)
> testData$x <- data.frame(testData$x)

Part A
Question
Tune several models on these data. An example is shown in the code. Which models appear to give the best performance? Does MARS select the informative predictors (those named X1–X5)?

Code 
```{r KNN Model}
library(caret) 
knnModel <- train(x = trainingData$x, y = trainingData$y, method = "knn", preProc = c("center", "scale"), tuneLength = 10) 
knnModel
```

```{r KNN Predictions}
knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set 
## performance values 
postResample(pred = knnPred, obs = testData$y)
```
```{r SVM Model}
svmModel <- train(x = trainingData$x,
                  y = trainingData$y,
                  method = "svmRadial",
                  tuneLength=10,
                  preProc = c("center", "scale"))
svmModel
svmPred <- predict(svmModel, newdata = testData$x)
postResample(pred = svmPred, obs = testData$y)
```

```{r Neural Net Model}
nnetGrid <- expand.grid(.decay=c(0, 0.01, 0.1, 0.5, 0.9),
                        .size=c(1, 10, 15, 20),
                        .bag=FALSE)

nnetModel <- train(x = trainingData$x,
                   y = trainingData$y,
                   method = "avNNet",
                   tuneGrid = nnetGrid,
                   preProc = c("center", "scale"),
                   trace=FALSE,
                   linout=TRUE,
                   maxit=500)
nnetModel
nnetPred <- predict(nnetModel, newdata = testData$x)
postResample(pred = nnetPred, obs = testData$y)
```
```{r Model Evaluation}
postResample(pred = nnetPred, obs = testData$y)
postResample(pred = svmPred, obs = testData$y)
postResample(pred = marsPred, obs = testData$y)
varImp(marsModel)
```
Response
SVM, Neural Networks and MARS models were used for testing performances. Below is R Code for the models. Comparing RMSE values of three models , It is clear that the MARS model performs better. The RMSE value is 1.32. Yes , It selects the informative predictors (X1-X5). R Code and results are below.


