---
title: "HW3"
subtitle: "Business Analytics and Data Mining"
author: "Zachary Palmore"
date: "3/26/2021"
output:
  rmdformats::html_clean:
    highlight: "monochrome"
    code_folding: "hide"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Assignment 3

___

## Overview 

In this homework assignment, we will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0). 

Our objective is to build a binary logistic regression model on the training data set to predict whether the neighborhood will be at risk for high crime levels. We will provide classifications and probabilities for the evaluation data set using our binary logistic regression model. We can only use the variables given to us (or variables that are derived from the variables provided).


## Introduction 

```{r message=FALSE}
# Packages
library(tidyverse)
library(reshape2)
library(ggpubr)
library(ggcorrplot)
library(kableExtra)
library(Amelia)
library(caret)
library(pROC)
theme_set(theme_minimal())
# Data
cdata <- read.csv("https://raw.githubusercontent.com/palmorezm/msds/main/621/HW3/crime-training-data_modified.csv")
cdata.eval <- read.csv("https://raw.githubusercontent.com/palmorezm/msds/main/621/HW3/crime-evaluation-data_modified.csv")
```

There are 466 observations in this data set with 13 different variables. Each observation is a statistical summary that indicates an attribute associated with a particular neighborhood within the major city. For example the first variable 'zn' contains the proportion of residential land zoned for lots that are over 25000 square feet. The first five observations of each variable are shown. 

```{r}
first5.tbl <- cdata[1:5,]
first5.tbl %>%
  kbl() %>%
  kable_styling()
```

Our target variable is labeled 'target' and it describes whether the neighborhood's crime rate is above the median or below it. Any or all of these variables could be used as our predictors except our target variable. In the next section we will explore the relationships between these predictors and review their interaction with our target to see which of the variables make the best predictors of crime rate. A full list of variable descriptions is also provided for reference. 

```{r}
vardesc <- data.frame(matrix(c(
'zn',	'proportion of residential land zoned for large lots (over 25000 square feet)',
'indus',	'proportion of non-retail business acres per suburb',
'chas',	'a dummy var. for whether the suburb borders the Charles River (1) or not (0)',
'nox',	'nitrogen oxides concentration (parts per 10 million)',
'rm',	'average number of rooms per dwelling',
'age',	'proportion of owner-occupied units built prior to 1940',
'dis',	'weighted mean of distances to five Boston employment centers',
'rad', 'index of accessibility to radial highways',
'tax',	'full-value property-tax rate per $10,000',
'ptratio',	'pupil-teacher ratio by town',
'lstat',	'lower status of the population (percent)',
'medv',	'median value of owner-occupied homes in $1000s',
'target',	'whether the crime rate is above the median crime rate (1) or not (0)'), byrow = TRUE, ncol = 2))
colnames(vardesc) <- c('Variable', 'Description')
kable(vardesc) %>% 
  kable_styling()
```

## Data Exploration

To determine which of the variables might make the best predictors we must first explore the data for potential sources of error. This includes, identifying and fixing missing values (if there are any), calculating and comparing averages, standard deviations, and other summary statistics. It would also be useful to evaluate how 

We will also look at this data set from 


Checking to see if any of the data are missing. 

```{r}
cdata.missingobservatons <- sum(is.na(cdata))
missmap(cdata, main = "Missing and Complete Observations")
```


None of the data are missing.

We can also visualize this to blue where there is data and red where there is a 'NA' or missing value for each variable. 



```{r}
summary.tbl <- summary(cdata)
summary.tbl 
```



```{r}
set.seed(72747)
indecies <- createDataPartition(cdata$target, p = .7, list = FALSE, times = 1)
train <- cdata[indecies,]
test <- cdata[-indecies,]
```


```{r}
train %>%
  select(-target) %>%
  summary() 
```



```{r}
train[1:12] %>%   
  melt() %>% 
  ggplot() +                      
    geom_histogram(aes(value, alpha = 2, fill = variable)) +
    facet_wrap(~ variable, scales = "free") + 
    geom_freqpoly(aes(value), bins=15, lty = 3) +
    labs(title = "Title", subtitle = "Subtitle") +
    theme(axis.title = element_blank(), 
          legend.position = "none", 
          plot.title = element_text(hjust = 0.5), 
          plot.title.position = "panel",
          plot.subtitle = element_text(hjust = 0.5)) 
```


```{r}
train[1:12] %>%   
  melt() %>% 
  ggplot() +                      
    geom_density(aes(value, alpha=.50, fill = variable)) +
    facet_wrap(~ variable, scales = "free") +
    labs(title = "Title", subtitle = "Subtitle") +
    theme(axis.title = element_blank(), 
          legend.position = "none", 
          plot.title = element_text(hjust = 0.5), 
          plot.title.position = "panel",
          plot.subtitle = element_text(hjust = 0.5)) 
```


```{r}
train %>% 
  melt() %>% 
  ggplot() + 
  geom_density(aes(value))
  
  t.train <- data.frame(t(train))
  t.train %>%
    melt() %>% 
    ggplot() +
    geom_point(aes(value, variable, color = variable)) + 
    theme(legend.position = "none",
          axis.title.x = element_blank(), 
          axis.text.y = element_blank()) 


m.train <- train %>% 
  melt()
f.train <- cbind(m.train, t.train)

f.train %>% 
  rename("predictor" = variable, 
         "m.vals" = value) %>% 
  melt()  %>% 
  ggplot() + 
  geom_density(aes(value, color = variable, alpha=.1)) + theme(legend.position = "none",
          axis.title.x = element_blank(), 
          axis.text.y = element_blank()) +
  geom_blank(aes(value), binwidth = 5)
  
f.train %>% 
  rename("predictor" = variable, 
         "m.vals" = value) %>% 
  melt()  %>% 
  group_by(predictor) %>%
  mutate(predictor.med = median(value), 
            predictor.mean = mean(value)) %>%
  ggplot() + 
  geom_point(aes(predictor.med, predictor, color = predictor)) 


f.train %>% 
  rename("predictor" = variable, 
         "m.vals" = value) %>% 
  melt()  %>% 
  group_by(predictor) %>%
  mutate(predictor.med = median(value), 
            predictor.mean = mean(value)) %>%
  ggplot() + 
  geom_point(aes(predictor.mean, predictor, color = predictor)) 

```



```{r}
train[1:12] %>% 
  melt() %>% 
  ggplot(, aes(variable, value)) + geom_violin(aes(variable, value, fill=variable)) + 
  geom_boxplot(aes(variable, value, color = variable)) + guides(color=FALSE, size=FALSE) +
  ggtitle("Boxplot of Predictors") +
  theme(axis.title = element_blank()) 
```


```{r}
# seperate into groups 
zimlp <- train[,c("zn", "indus", "medv", "lstat", "ptratio")]
ta <- train[,c("tax", "age")]
rdr <- train[,c("rad", "dis", "rm")]
nc <- train[,c("nox", "chas")]
```



```{r}
viobox.zimlp <- zimlp %>% 
  melt() %>% 
  ggplot() + 
  geom_violin(aes(variable, value, fill=variable, fatten=2)) +
  geom_boxplot(aes(variable, value, color = variable, alpha = .90)) +
  xlab("Value") + ylab("Variable") + 
  theme(legend.position = "none", axis.title = element_blank()) +
  coord_flip()
viobox.ta <- ta %>% 
  melt() %>% 
  ggplot() + 
  geom_violin(aes(variable, value, fill=variable, fatten=2)) +
  geom_boxplot(aes(variable, value, color = variable, alpha = .90)) +
  xlab("Value") + ylab("Variable") + 
  theme(legend.position = "none", axis.title = element_blank()) +
  coord_flip()
viobox.rdr <- rdr %>% 
  melt() %>% 
  ggplot() + 
  geom_violin(aes(variable, value, fill=variable, fatten=2)) +
  geom_boxplot(aes(variable, value, color = variable, alpha = .90)) +
  xlab("Value") + ylab("Variable") + labs(title = "Predictor Summary Plots",
                                          subtitle = "Using Mean, Density, & Spread") + 
  theme(legend.position = "none", axis.title = element_blank()) +
  coord_flip()
viobox.nc <- nc %>% 
  melt() %>% 
  ggplot() + 
  geom_violin(aes(variable, value, fill=variable, fatten=2)) +
  geom_boxplot(aes(variable, value, color = variable, alpha = .90)) +
  xlab("Value") + ylab("Variable") + 
  theme(legend.position = "none", axis.title = element_blank()) +
  coord_flip()
ggarrange(viobox.rdr, viobox.zimlp, viobox.nc, viobox.ta) 
```



```{r}
# Function to calculate and set pointrange
xysdu <- function(x) {
   m <- mean(x)
   ymin <- m - sd(x)
   ymax <- m + sd(x)
   return(c(y = m, ymin = ymin, ymax = ymax))
}

colors <- c("Median" = "Red", "Mean" = "Black")
ptrng.zimlp <- zimlp %>% 
  melt() %>% 
  ggplot(aes(variable, value)) + coord_flip() + 
  stat_summary(fun.data=xysdu, geom = "Pointrange", shape=16, size=.5, color="black") +
  stat_summary(fun.y=median, geom="point", shape=16, size=2, color="red") + 
  theme(legend.position = "None", axis.title.x = element_blank(), axis.title.y = element_blank())
ptrng.rdr <- rdr %>% 
  melt() %>% 
  ggplot(aes(variable, value)) + coord_flip() + 
  stat_summary(fun.data=xysdu, geom = "Pointrange", shape=16, size=.5, color="black") +
  stat_summary(fun.y=median, geom="point", shape=16, size=2, color="red") + 
  labs(title = "Characteristic Outlier Analysis", 
       subtitle = "By Preditor Mean & Median") + 
  scale_color_manual(values = colors) +
  theme(legend.position = "Bottom", axis.title.x = element_blank(), axis.title.y = element_blank())
ptrng.ta <- ta %>% 
  melt() %>% 
  ggplot(aes(variable, value)) + coord_flip() + 
  stat_summary(fun.data=xysdu, geom = "Pointrange", shape=16, size=.5, color="black") +
  stat_summary(fun.y=median, geom="point", shape=16, size=2, color="red") + 
  theme(legend.position = "None", axis.title.x = element_blank(), axis.title.y = element_blank())
ptrng.nc <- nc %>% 
  melt() %>% 
  ggplot(aes(variable, value)) + coord_flip() + 
  stat_summary(fun.data=xysdu, geom = "Pointrange", shape=16, size=.5, color="black") +
  stat_summary(fun.y=median, geom="point", shape=16, size=2, color="red") + 
  theme(legend.position = "None", axis.title.x = element_blank(), axis.title.y = element_blank())
ggarrange(ptrng.rdr, ptrng.zimlp, ptrng.nc, ptrng.ta) 
```




```{r}
train %>% 
  cor() %>% 
  ggcorrplot(method = "square", type="upper", ggtheme = ggplot2::theme_minimal, legend.title = "Influence") + coord_flip()
```




## Data Preparation

Describe how you have transformed the data by changing the original variables or creating new variables. If you 
did transform the data or create new variables, discuss why you did this. Here are some possible transformations.
a. Fix missing values (maybe with a Mean or Median value)
b. Create flags to suggest if a variable was missing
c. Transform data by putting it into buckets
d. Mathematical transforms such as log or square root (or use Box-Cox)
e. Combine variables (such as ratios or adding or multiplying) to create new variables


```{r}
baseline <- glm(target ~ nox, family = binomial(link = "logit"), train)
summary(baseline)
```


## Build Models

Using the training data, build at least three different binary logistic regression models, using different variables (or 
the same variables with different transformations). You may select the variables manually, use an approach such 
as Forward or Stepwise, use a different approach, or use a combination of techniques. Describe the techniques 
you used. If you manually selected a variable for inclusion into the model or exclusion into the model, indicate why 
this was done.
Be sure to explain how you can make inferences from the model, as well as discuss other relevant model output.
Discuss the coefficients in the models, do they make sense? Are you keeping the model even though it is counter 
intuitive? Why? The boss needs to know.


## Select Models

Decide on the criteria for selecting the best binary logistic regression model. Will you select models with slightly 
worse performance if it makes more sense or is more parsimonious? Discuss why you selected your models. 
For the binary logistic regression model, will you use a metric such as log likelihood, AIC, ROC curve, etc.? Using 
the training data set, evaluate the binary logistic regression model based on (a) accuracy, (b) classification error
rate, (c) precision, (d) sensitivity, (e) specificity, (f) F1 score, (g) AUC, and (h) confusion matrix. Make predictions 
using the evaluation data set



## Conclusion

