---
title: "BlogEntry1"
author: "Zachary Palmore"
date: "5/17/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## The Normality Assumption

In linear regression we often attempt to predict based on some data that represents reality. For this week's post, we will take a look at how to assess an important assumption that must be made to predict accurately using linear regression. To understand how to assess normality we use data from an irrigation experiment in which a sprayer nozzle delivers a certain amount of water per trial. Our goal is to estimate the amount of water delivered per trial and predict how much will be delivered at a 20, 50, and 100 trials of this sprayer nozzle. A sample of this data is provided below. 

```{r data}
sprays <- read.csv("https://raw.githubusercontent.com/palmorezm/misc/master/Sprays.csv")
library(tidyverse)
vol <- sample_n(sprays, 400, replace = T)
colnames(vol) <- c("Trial","Sprays","Diameter","Height")
vol <- vol %>% 
  mutate(volume = pi*(Diameter)*Height) %>% 
  mutate(deliver = volume/1000)
```

Now, this experiment was set up such that, each trial contains certain number of sprays. In this case, 




Normality, along with three other assumptions of independent observations, linearity, and homoscedasticity, allows us to take advatange some statistical principles to better intepret future data points. 



```{r}
# Expectation
vol %>% 
  ggplot(aes(Sprays, deliver)) + geom_point()
# Nonlinear, not normal?
vol %>% 
  ggplot(aes(Sprays, log(deliver))) + geom_point()

vol %>% 
  ggplot(aes(Sprays, deliver^9)) + geom_point()
```








