---
title: "HW4"
author: "Zachary Palmore"
date: "4/17/2021"
output: pdf_document
subtitle: "Business Analytics and Data Mining"
header-includes: 
- \newcommand{\bcenter}{\begin{center}}
- \newcommand{\ecenter}{\end{center}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\bcenter

# Assignment 4

\ecenter

___

```{r warning=F, message=F}
# Packages
library(tidyverse)
library(kableExtra)
library(ggcorrplot)
library(reshape2)
library(bestNormalize)
library(caret)
library(MASS)
library(pROC)
library(stats)
library(ROCR)
theme_set(theme_minimal())
```

\newpage

## Purpose 

In this homework assignment, we will explore, analyze and model a data set containing approximately 8000 records representing a customer at an auto insurance company. Each record has two response variables. The first response variable, TARGET_FLAG, is a 1 or a 0. A “1” means that the person was in a car crash. A zero means that the person was not in a car crash. The second response variable is TARGET_AMT. This value is zero if the person did not crash their car. But if they did crash their car, this number will be a value greater than zero.

Our objective is to build multiple linear regression and binary logistic regression models on the training data to predict the probability that a person will crash their car and also the amount of money it will cost if the person does crash their car. We can only use the variables given (or variables derived from the variables provided). Below is a short description of the variables of interest in the data set:




```{r}
# short descriptions of variables as table from matrix
vardesc <- data.frame(matrix(c(
'INDEX',	'Identification variable',
'TARGET_FLAG',	'Was car in a crash? 1 = Yes, 0 = No',
'TARGET_AMT',	'Cost of car crash',
'AGE',	'Age of driver',
'BLUEBOOK',	'Value of vehicle',
'CAR_AGE',	'Vehicle age',
'CAR_TYPE',	'Type of car',
'CAR_USE', 'Main purpose the vehicle is used for',
'CLM_FREQ',	'Number of claims filed in past five years',
'EDUCATION',	'Maximum education level',
'HOMEKIDS',	'Number of children at home',
'HOME_VAL',	'Value of driver\'s home',
'INCOME',	'Annual income of the driver',
'JOB',	'Type of job by standard collar categories',
'KIDSDRIV',	'Number of children who drive',
'MSTATUS',	'Marital status',
'MVR_PTS',	'Motor vehicle inspection points',
'OLDCLAIM',	'Total claims payout in past five years',
'PARENT1',	'Single parent status',
'RED_CAR',	'1 if car is red, 0 if not',
'REVOKED',	'License revoked in past 7 years status',
'SEX',	'Driver gender',
'TIF',	'Time in force',
'TRAVETIME',	'Distance to work in minutes',
'URBANICITY',	'Category of how urban the area the driver lives is',
'YOJ', 'Number of years on the job'
),  byrow = TRUE, ncol = 2))
colnames(vardesc) <- c('Variable', 'Description')
kbl(vardesc, booktabs = T, caption = "Variable Descriptions") %>%
  kable_styling(latex_options = c("striped", "HOLD_position"), full_width = F)
```







___


\newpage


## Introduction

There are [ ] observations of [ ] variables in this data set.  


```{r}
tdata <- read.csv(
  "https://raw.githubusercontent.com/palmorezm/msds/main/621/HW4/insurance_training_data.csv")
edata <- read.csv(
  "https://raw.githubusercontent.com/palmorezm/msds/main/621/HW4/insurance-evaluation-data.csv")
```



```{r}
initialobs <- tdata[1:4,]
kbl(t(initialobs), booktabs = T, caption = "Initial Observations") %>%
  kable_styling(latex_options = c("striped", "HOLD_position"), full_width = F) %>%
  add_header_above(c(" ", " ", "Row Number", " ", " ")) %>%
  footnote(c("Includes the first four observations of all variables in the data"))
```



___



\newpage



## Data Exploration

Describe the size and the variables in the insurance training data set. Consider that too much detail will cause a 
manager to lose interest while too little detail will make the manager consider that you aren’t doing your job. Some 
suggestions are given below. Please do NOT treat this as a check list of things to do to complete the assignment. 
You should have your own thoughts on what to tell the boss. These are just ideas.
a. Mean / Standard Deviation / Median
b. Bar Chart or Box Plot of the data
c. Is the data correlated to the target variable (or to other variables?)
d. Are any of the variables missing and need to be imputed “fixed”?


Before we delve into the nitty gritty of this data set, we should consider what effect each of these variables might exert on the outcome. Since there are two targets of different types, and thus two models (one logistic classifier and one regression) there could be an influence on either or both models. As we understand it, the theoretical effects of each variable are recorded in the table below.

```{r}
# theoretical effects
vareffects <- data.frame(matrix(c(
'INDEX',	'None',
'TARGET_FLAG',	'None',
'TARGET_AMT',	'None',
'AGE',	'Youngest and Oldest may have higher risk of accident',
'BLUEBOOK',	'Unknown on probability of collision but correlated with payout',
'CAR_AGE',	'Unknown on probability of collision but correlated with payout',
'CAR_TYPE',	'Unknown on probability of collision but correlated with payout',
'CAR_USE', 'Commerical vehicles might increase risk of accident',
'CLM_FREQ',	'Higher claim frequency increases likelihood of future claims',
'EDUCATION',	'Theoretically higher education levels lower risk',
'HOMEKIDS',	'Unknown',
'HOME_VAL',	'Theoretically home owners reduce risk due to more responsible driving',
'INCOME',	'Theoretically wealthier drivers have fewer accidents',
'JOB',	'Theoretically white collar+ jobs are safer',
'KIDSDRIV',	'Increased risk of accident from inexperienced driver',
'MSTATUS',	'Theoretically married people drive safer',
'MVR_PTS',	'Increased risk of accident',
'OLDCLAIM',	'Increased risk of higher payout with previous payout',
'PARENT1',	'Unknown',
'RED_CAR',	'Theoretically increased risk of accident based on urban legend',
'REVOKED',	'Increased risk of accident if revoked',
'SEX',	'Theoretically increased risk of accident for women based on urban legend',
'TIF',	'Decreased risk for those who have greater loyalty',
'TRAVETIME',	'Longer distances increase risk of accident',
'URBANICITY',	'The more urban the area the greater the risk of accident',
'YOJ', 'Decreased risk for those with greater longevity'
),  byrow = TRUE, ncol = 2))
colnames(vareffects) <- c('Variable', 'Effect')
kbl(vareffects, booktabs = T, caption = "Theoretical Variable Effects") %>%
  kable_styling(latex_options = c("striped", "HOLD_position"), full_width = F)
```



This table considers the effects of both models but they are only theoretical and may not necessarily reflect the true influence. We will evaluate these directly in the model selection process. For now, they will serve as general baseline expectations for exploration and preparation. We continue by exploring the data to determine where munging may be necessary. 

Unfortunately, this data needs work before we are able to make visualizations and contemplate improvements to the model. We consider the amount of missing values in relative proportions to each variable, followed by their respective data types, an example observation of each type, and the quantity of unique factors to each variable. This will help narrow down what is needed to prepare the data for modeling. Results are shown in the table:



```{r}
tdata.nas <- lapply(tdata, function(x) sum(is.na(x)))
tdata.len <- lapply(tdata, function(x) length(x))
tdata.permis <- lapply(tdata, function(x) round(sum(is.na(x))/length(x)*100, 1))
tdata.types <- lapply(tdata, function(x) class(x))
tdata.firstob <- lapply(tdata, function(x) head(x, 1))
tdata.uniques <- lapply(tdata, function(x) length(unique(factor(x))))
tdata.tbl.natypes <- cbind(tdata.nas, tdata.len, tdata.permis, tdata.types, tdata.firstob, tdata.uniques) 
colnames(tdata.tbl.natypes) <- c("Missing", "Total", "%", "Data Type", "Example", "Factors")
kbl(tdata.tbl.natypes, booktabs = T, caption = "Data Characteristics") %>%
  kable_styling(latex_options = c("striped", "HOLD_position"), full_width = F)
```



Three variables contain incomplete records including ‘AGE’, ‘YOJ’,  and ‘CAR_AGE’ with 0.1%, 5.6%, and 6.2% of their data missing. Theoretically each variable would have 8161 total observations as noted in the table. The data types are either integer or numeric and the examples display what the type looks like for easy referencing. A calculation of the unique factors for each variable is included to gauge whether converting to a factor data type would be right for the variable and count the number of unique values to each. These are major concerns. 

Minima, quartiles, averages, and maximums were computed to compare the numeric integer variables. Although the order of the variables remains the same as in the previous table, we added a missing values column with the row identifier ‘NA’ to count the number missing for tracking purposes. We put this together in a table called Data Characteristics. Of course, several of the variables will need to be altered before we can evaluate if the data makes sense in a real-life scenario. These are shown as NA in the table. 



```{r}
tdata.summary.tbl <- summary(tdata)
kbl(t(tdata.summary.tbl), booktabs = T, caption = "Data Characteristics") %>%
  kable_styling(latex_options = c("striped", "scale_down", "hold_position"), full_width = F)
```







```{r}
tdata %>% 
  select_if(is.numeric) %>% 
  gather %>% 
  ggplot() +
  facet_wrap(~ key, scales = "free") +
  geom_density(aes(value, color = value, fill = key, alpha = .5)) + theme(axis.title = element_blank(), legend.position = "none") + ggtitle("Numeric Variable Density") + theme(plot.title = element_text(hjust = 0.5))
```



```{r}
tdata %>% 
  select_if(is.numeric) %>% 
  gather %>% 
  ggplot(aes(value, key)) +
  facet_wrap(~ key, scales = "free") +
  geom_violin(aes(color = key, alpha = 1)) + 
  geom_boxplot(aes(fill = key, alpha = .5), notch = TRUE, size = .1, lty = 3) +  
  stat_summary(fun.y = mean, geom = "point",
               shape = 8, size = 1.5, color = "#000000") + 
  theme(axis.text = element_blank(), 
        axis.title = element_blank(), 
        legend.position = "none") + 
  ggtitle("Numeric Variable KDE & Distribution") + 
  theme(plot.title = element_text(hjust = 0.5)) 
```



```{r}
tdata %>% 
  select_if(is.integer) %>%
  gather() %>% 
  filter(value == 0 | 1) %>% 
  group_by(key) %>% 
  ggplot() +
  facet_wrap(~ key, scales = "free") +
  geom_bar(aes(value, color = value, fill = key, alpha = .5)) + theme(axis.title = element_blank(), legend.position = "none") + ggtitle("Integer Frequencies") + theme(plot.title = element_text(hjust = 0.5))
```


```{r}
tdata %>% 
  dplyr::select(TARGET_FLAG, MVR_PTS, CLM_FREQ, HOMEKIDS, KIDSDRIV, TIF) %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") + 
  geom_bar(aes(value, color = key, fill = key, alpha = .5)) + theme(axis.title = element_blank(), legend.position = "none") + ggtitle("Select Integer Frequencies") + theme(plot.title = element_text(hjust = 0.5))
```



```{r}
tdata %>%
  select_if(is.numeric) %>% 
  cor() %>% 
  ggcorrplot(method = "circle", type="upper", 
             ggtheme = ggplot2::theme_minimal, legend.title = "Influence") + coord_flip() 
```
















___



\newpage



## Data Preparation

Describe how you have transformed the data by changing the original variables or creating new variables. If you
did transform the data or create new variables, discuss why you did this. Here are some possible transformations.
a. Fix missing values (maybe with a Mean or Median value)
b. Create flags to suggest if a variable was missing
c. Transform data by putting it into buckets
d. Mathematical transforms such as log or square root (or use Box-Cox)
e. Combine variables (such as ratios or adding or multiplying) to create new variables



```{r}
# Select character variables
chars <- dplyr::select(tdata, where(is.character))
# Use function to extract dollars
to_num <- function(x){
  x <- as.character(x)
  x <- gsub(",", "", x)
  x <- gsub("\\$", "", x)
  as.numeric(x)
}
# Specify those dollar variables 
income.values <- to_num(chars$INCOME)
home.values <- to_num(chars$HOME_VAL)
bluebook.values <- to_num(chars$BLUEBOOK)
oldclaim.values <- to_num(chars$OLDCLAIM)
concept_df <- as.data.frame(cbind(income.values, 
                    home.values, 
                    bluebook.values, 
                    oldclaim.values))
income.values.stat <- to_num(chars$INCOME)
home.values.stat <- to_num(chars$HOME_VAL)
bluebook.values.stat <- to_num(chars$BLUEBOOK)
oldclaim.values.stat <- to_num(chars$OLDCLAIM)
# impute median values for missing variables
income.values[is.na(income.values)] <- 
  median(income.values, na.rm = TRUE)
home.values[is.na(home.values)] <- 
  median(home.values, na.rm = TRUE)
bluebook.values[is.na(bluebook.values)] <- 
  median(bluebook.values, na.rm = TRUE)
oldclaim.values[is.na(oldclaim.values)] <- 
  median(oldclaim.values, na.rm = TRUE)
# Recombine into data frame
dollar.values <- 
  data.frame(cbind(income.values, 
                   home.values, 
                   bluebook.values, 
                   oldclaim.values))
dollar.values.stats <- 
  data.frame(cbind(income.values.stat, 
                   home.values.stat, 
                   bluebook.values.stat,
                   oldclaim.values.stat))
# Join with training data
tdata <- data.frame(cbind(tdata, dollar.values))
# Check the difference
dollar.values.tbl <- summary(dollar.values)
dollar.values.stats.tbl <- summary(dollar.values.stats)
kbl(dollar.values.tbl, booktabs = T, caption = "Imputed Summary Statistics") %>%
kable_styling(latex_options = c("striped", "hold_position"), full_width = F)
```


```{r}
kbl(dollar.values.stats.tbl, booktabs = T, caption = "Original Summary Statistics") %>%
kable_styling(latex_options = c("striped", "hold_position"), full_width = F)
```



```{r}
# Covert categorical variables to factors 
factors <- tdata %>% 
  dplyr::select("PARENT1", 
         "MSTATUS", 
         "SEX", 
         "EDUCATION", 
         "JOB", 
         "CAR_USE",
         "CAR_TYPE",
         "RED_CAR", 
         "REVOKED", 
         "URBANICITY") 
factors <- data.frame(lapply(factors, function(x) as.factor(x)))
factors <- factors %>% 
  rename("parent1" = "PARENT1", 
         "mstatus" = "MSTATUS", 
         "sex" = "SEX", 
         "education" = "EDUCATION", 
         "job" = "JOB", 
         "car_use" = "CAR_USE", 
         "car_type" = "CAR_TYPE",
         "red_car" = "RED_CAR", 
         "revoked" = "REVOKED", 
         "urbanicity" = "URBANICITY")
tdata <- cbind(tdata, factors)
```



```{r}
# Exclude unrealistic values
tdata <- tdata %>% 
  mutate(car_age = ifelse(CAR_AGE<0, NA, CAR_AGE))
summary(tdata$car_age)
summary(tdata$CAR_AGE)
```


```{r include=F}
full41 <- tdata
full41
full41[c(2:7, 15, 18, 22, 24, 27:41)]
```


```{r}
# Drop INDEX and other unnecessary columns
tdata <- tdata %>% 
  dplyr::select("TARGET_FLAG",
         "TARGET_AMT",
         "KIDSDRIV",
         "AGE",
         "HOMEKIDS",
         "YOJ",
         "TRAVTIME",
         "TIF",
         "CLM_FREQ",
         "MVR_PTS",
         "income.values", 
         "home.values",
         "bluebook.values",
         "oldclaim.values",
         "parent1", 
         "mstatus",
         "sex",
         "education",
         "job",
         "car_use",
         "car_age",
         "car_type",
         "red_car", 
         "revoked",
         "urbanicity")
# Check total variables present
length(colnames(tdata))
```






```{r}
# More imputation
tdata$AGE[is.na(tdata$AGE)] <-
  median(tdata$AGE, na.rm = T)
tdata$YOJ[is.na(tdata$YOJ)] <-
  median(tdata$YOJ, na.rm = T)
tdata$car_age[is.na(tdata$car_age)] <- 
  median(tdata$car_age, na.rm = T)
sum(is.na(tdata))
```


```{r}
tdata %>% 
  dplyr::select(is.factor) %>%
  dplyr::select("car_type", "education", "job") %>% 
  gather() %>% 
  ggplot(aes(value)) + 
  facet_wrap(~ key, nrow = 3, scales = "free") + 
  geom_bar(aes(, fill = key )) + theme(axis.title = element_blank(), axis.text.x = element_blank(), legend.position = "none") + coord_flip() + ggtitle("Nonbinary Classifiers") + theme(plot.title = element_text(hjust = 0.5))
```

```{r}
tdata %>% 
  dplyr::select(is.factor) %>%
  dplyr::select("car_use", 
         "mstatus", 
         "parent1", 
         "red_car",
         "revoked",
         "sex",
         "urbanicity") %>% 
  gather() %>% 
  ggplot(aes(value)) + 
  facet_wrap(~ key, scales = "free") + 
  geom_bar(aes(, fill = key )) + theme(axis.title = element_blank(), axis.text.x = element_blank(), legend.position = "none") + coord_flip() + ggtitle("Binary Classifiers Counts") + theme(plot.title = element_text(hjust = 0.5))
```





```{r}
tdata %>% 
  select_if(is.numeric) %>%
  gather() %>% 
  ggplot(aes(key)) + 
  facet_wrap(~ key, scales = "free") + 
  geom_boxplot(aes(key, value, fill = key, alpha = .5)) + theme(axis.title = element_blank(), axis.text.x = element_blank(), legend.position = "none") + ggtitle("Numeric Distributions") + theme(plot.title = element_text(hjust = 0.5))
```


```{r}
# New features
tdata <- tdata %>% 
  mutate(city = ifelse(urbanicity == "Highly Urban/ Urban", 0, 1)) %>% 
  mutate(young = ifelse(AGE < 25, 1, 0)) %>% 
  mutate(clean_rec = ifelse(MVR_PTS == 0, 1, 0)) %>%
  mutate(previous_accident = ifelse(CLM_FREQ == 0 & oldclaim.values == 0, 0, 1)) %>% 
  mutate(educated = ifelse(education %in% c("Bachelors", "Masters", "PhD"), 1, 0)) %>% 
  mutate(avg_claim = ifelse(CLM_FREQ > 0, oldclaim.values/CLM_FREQ, 0))
```



```{r}
# Convert to factors 
tdata$city <- as.factor(tdata$city)
tdata$young <- as.factor(tdata$young)
tdata$clean_rec <- as.factor(tdata$clean_rec)
tdata$previous_accident <- as.factor(tdata$previous_accident)
tdata$educated <- as.factor(tdata$educated)
```



```{r}
tdata[26:31] %>% 
  select_if(is.factor) %>% 
  gather() %>% 
  ggplot(aes(value)) + 
  facet_wrap(~key, scales = "free") + 
  geom_bar(aes(fill = key, alpha = .5)) + theme(legend.position = "none", axis.title = element_blank()) + ggtitle("New Features") + theme(plot.title = element_text(hjust = 0.5))
```




```{r}
# Produce recommended transformations
bestNorms <- tdata[1:11,1:16]
df <- tdata %>% 
  select_if(is.numeric)
for (i in colnames(df)) {
  bestNorms[[i]] <- bestNormalize(df[[i]],
                                  allow_orderNorm = FALSE,
                                  out_of_sample =FALSE)
}
```


```{r}
# Continue focusing on realistic values
accident_costs <- tdata$TARGET_AMT[tdata$TARGET_AMT>.0]
```



```{r}
# Focus on selected variables 
bestNorms$target_amt$chosen_transform
tdata$target_amt <- scale(log(tdata$TARGET_AMT + 1))
tdata %>% 
  dplyr::select(where(is.numeric)) %>%
  gather %>% 
  ggplot() +
  facet_wrap(~ key, scales = "free") +
  geom_density(aes(value, color = value, fill = key, alpha = .5)) + theme(axis.title = element_blank(), legend.position = "none") + ggtitle("Numeric Variable Density") + theme(plot.title = element_text(hjust = 0.5))
```


```{r}
tdata %>% 
  dplyr::select(where(is.numeric)) %>%
  dplyr::select("TARGET_AMT","target_amt") %>%
  gather %>% 
  ggplot() +
  facet_wrap(~ key, scales = "free") +
  geom_density(aes(value, color = value, fill = key, alpha = .5)) + theme(axis.title = element_blank(), legend.position = "none") + ggtitle("Numeric Variable Density") + theme(plot.title = element_text(hjust = 0.5))
```



```{r}
# Split 70-30 training test
set.seed(1102)
tindex <- createDataPartition(tdata$TARGET_FLAG, p = .7, list = FALSE, times = 1)
train <- tdata[tindex,]
test <- tdata[-tindex,]
rindex <- tdata %>%
  filter(TARGET_FLAG == 1)
reg.tindex <- createDataPartition(rindex$TARGET_AMT, p = .7, list = FALSE, times = 1)
reg.train <- rindex[reg.tindex,]
reg.test <- rindex[-reg.tindex,]
```






___


\newpage


## Model Building

Using the training data set, build at least two different multiple linear regression models and three different binary 
logistic regression models, using different variables (or the same variables with different transformations). You 
may select the variables manually, use an approach such as Forward or Stepwise, use a different approach such 
as trees, or use a combination of techniques. Describe the techniques you used. If you manually selected a 
variable for inclusion into the model or exclusion into the model, indicate why this was done.

Discuss the coefficients in the models, do they make sense? For example, if a person has a lot of traffic tickets, 
you would reasonably expect that person to have more car crashes. If the coefficient is negative (suggesting that 
the person is a safer driver), then that needs to be discussed. Are you keeping the model even though it is counter 
intuitive? Why? The boss needs to know.


```{r}
model1 <- glm(TARGET_FLAG ~ previous_accident, 
              family = binomial(link = "logit"), train)
summary(model1)
```


```{r}
model2 <- glm(TARGET_FLAG ~ previous_accident + 
                city + young + clean_rec + 
                educated, family = binomial(link = "logit"), train)
summary(model2)
```


```{r}
model3 <- glm(TARGET_FLAG ~ previous_accident + 
                city + mstatus + income.values + 
                sex + car_use + educated + KIDSDRIV + 
                revoked, family = binomial(link = "logit"), 
              train)
summary(model3)
```


```{r}
model4 <- lm(target_amt ~ ., train) 
summary(model4)
```


```{r}
model5 <- lm(target_amt ~ income.values +
               home.values + bluebook.values + 
               oldclaim.values + avg_claim, 
             train) 
summary(model5)
```



```{r include=F, eval=F}
model7 <- lm(target_amt ~ 
               income.values + 
               home.values + 
               bluebook.values + 
               oldclaim.values + 
               # 2nd Degree
               ident(avg_claim, train^2) + 
               I(income.values^2) + 
               I(home.values^2) + 
               I(bluebook.values^2) + 
               I(oldclaim.values^2) + 
               I(avg_claim, train^2) + 
               # 3rd Degree
               I(avg_claim, train^3) + 
               I(income.values^3) + 
               I(home.values^3) + 
               I(bluebook.values^3) + 
               I(oldclaim.values^3) + 
               I(avg_claim, train^3), train
             ) 
pm <- stepAIC(model7, trace = F, direction = "both")
p <- summary(pm)$call
pm <- lm(p[2], df)
summary(pm)
```



```{r}
model6 <- lm(target_amt ~ . -TARGET_AMT -TARGET_FLAG, train) 
pm <- stepAIC(model6, trace = F, direction = "both")
summary(pm)
```



___


\newpage


## Model Selection

Decide on the criteria for selecting the best multiple linear regression model and the best binary logistic regression 
model. Will you select models with slightly worse performance if it makes more sense or is more parsimonious? 
Discuss why you selected your models. 
For the multiple linear regression model, will you use a metric such as Adjusted R2
, RMSE, etc.? Be sure to 
explain how you can make inferences from the model, discuss multi-collinearity issues (if any), and discuss other 
relevant model output. Using the training data set, evaluate the multiple linear regression model based on (a) 
mean squared error, (b) R2
, (c) F-statistic, and (d) residual plots. For the binary logistic regression model, will you 
use a metric such as log likelihood, AIC, ROC curve, etc.? Using the training data set, evaluate the binary logistic 
regression model based on (a) accuracy, (b) classification error rate, (c) precision, (d) sensitivity, (e) specificity, (f) 
F1 score, (g) AUC, and (h) confusion matrix. Make predictions using the evaluation data set.



```{r}
# Calculate predicted values
# Classifier Model
mod1.pred <- predict.glm(model1, test)
mod2.pred <- predict.glm(model2, test)
mod3.pred <- predict.glm(model3, test)
# Regression Model
mod4.pred <- predict(model4, test, interval = "prediction")
mod5.pred <- predict(model5, test, interval = "prediction")
mod6.pred <- predict(model6, test, interval = "prediction")
```



```{r}
modstat <- function(model, test, target = "TARGET_FLAG", threshold = 0.5){
  test$new <- ifelse(predict.glm(model, test, "response") >= threshold, 1, 0)
  cm <- confusionMatrix(factor(test$new), factor(test[[target]]), "1")
  df <- data.frame(obs = test$TARGET_FLAG, predicted = test$new, probs = predict(model, test))
  Pscores <- prediction(df$probs, df$obs)
  AUC <- performance(Pscores, measure = "auc")@y.values[[1]]
  pscores <- performance(Pscores, "tpr", "fpr")
  plot(pscores,main="ROC Curve", sub = paste0("AUC: ", round(AUC, 3)))
  results <- paste(cat("F1 = ", cm$byClass[7], " "), cm)
  return(results)
}
```




```{r}
modstat(model1, test)
```







```{r}
modstat(model2, test)
```



```{r}
modstat(model3, test)
```



```{r}
modstat(model4, test)
```



```{r}
modstat(model5, test)
```



```{r}
modstat(model6, test)
```



## Conclusion


